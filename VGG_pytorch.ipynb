{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85b9cf37",
   "metadata": {},
   "source": [
    "## VGG\n",
    "\n",
    "Building on the work of AlexNet, VGG focuses on another crucial aspect of Convolutional Neural Networks (CNNs), depth. It was developed by Simonyan and Zisserman. It normally consists of 16 convolutional layers but can be extended to 19 layers as well (hence the two versions, VGG-16 and VGG-19). All the convolutional layers consists of 3x3 filters. You can read more about the network in the official paper [here](arxiv.org/pdf/1409.1556.pdf)\n",
    "\n",
    "### Building the VGG Architecture:\n",
    "\n",
    "- Import the necessary libraries, such as PyTorch or TensorFlow.\n",
    "- Define VGG blocks, which consist of convolutional layers followed by max-pooling.\n",
    "- Create the complete VGG architecture by stacking multiple VGG blocks.\n",
    "- Add fully connected layers at the end for classification.\n",
    "- Key Concepts and Layers in VGG:\n",
    "\n",
    "### VGG architectures have a straightforward structure with repeated convolutional blocks.\n",
    "- Convolutional layers use small 3x3 kernels with padding.\n",
    "- Max-pooling layers downsample feature maps.\n",
    "- VGG models come in different variants, such as VGG-16 and VGG-19.\n",
    "- The final fully connected layers perform classification.\n",
    "\n",
    "### Code Examples and Implementations:\n",
    "- Use code to define VGG blocks and the complete VGG architecture.\n",
    "- Specify the number of layers, channels, and classes as needed.\n",
    "- Implement forward propagation to process input data.\n",
    "\n",
    "### Training and Evaluation:\n",
    "\n",
    "- Train the VGG model on a dataset of your choice.\n",
    "- Evaluate model performance using test images.\n",
    "- Monitor metrics like accuracy, loss, and more.\n",
    "\n",
    "### Further Exploration and Improvements:\n",
    "- Experiment with different VGG variants (e.g., VGG-16, VGG-19).\n",
    "- Fine-tune the model on specific tasks or datasets.\n",
    "- Explore transfer learning by using pretrained VGG models.\n",
    "- Consider architectural modifications for specialized tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb3f213",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import list_models, get_model, get_model_weights, get_weight\n",
    "import transformers\n",
    "\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3,4\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e0681f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionNet(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int,\n",
    "                 kernel_size: int, stride: int=1, dilation: int=1,\n",
    "                 bias: bool=True, padding: int=1, padding_mode: str='zeros', add_conv1: bool=True):\n",
    "        super().__init__()\n",
    "        self.add_conv1 = add_conv1\n",
    "        self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size, stride,\n",
    "                               dilation=dilation, bias=bias, padding=padding, padding_mode=padding_mode)\n",
    "        self.conv2 = nn.Conv2d(in_channels, out_channels, kernel_size, stride,\n",
    "                               dilation=dilation, bias=bias, padding=padding, padding_mode=padding_mode)\n",
    "        if add_conv1:\n",
    "            self.one_kernel = nn.Conv2d(out_channels, out_channels, 1, bias=bias)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(in_channels)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(out_channels)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        if self.add_conv1:\n",
    "            x = self.one_kernel(x)\n",
    "        return self.activation(x)\n",
    "\n",
    "\n",
    "class Pooling(nn.Module):\n",
    "    def __init__(self, kernel_size, stride=1, padding=0):\n",
    "        super().__init__()\n",
    "        self.pooler = nn.MaxPool2d(kernel_size, stride=stride, padding=padding)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.pooler(x)\n",
    "\n",
    "\n",
    "class FCNet(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, bias: bool=True, inter_dim: int=4096, number: int=3):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential()\n",
    "        if number > 1 and inter_dim:\n",
    "            self.layers.append(nn.Linear(input_dim, inter_dim, bias=bias))\n",
    "            self.layers.append(nn.ReLU())\n",
    "            for _ in range(number-2):\n",
    "                self.layers.append(nn.Dropout(0.2))\n",
    "                self.layers.append(nn.Linear(inter_dim, inter_dim, bias=bias))\n",
    "                self.layers.append(nn.ReLU())\n",
    "            self.layers.append(nn.Linear(inter_dim, num_classes, bias=bias))\n",
    "            self.layers.append(nn.ReLU())\n",
    "        elif number > 1:\n",
    "            for _ in range(number-1):\n",
    "                self.layers.append(nn.Dropout(0.2))\n",
    "                self.layers.append(nn.Linear(input_dim, input_dim, bias=bias))\n",
    "                self.layers.append(nn.ReLU())\n",
    "            self.layers.append(nn.Linear(inter_dim, num_classes, bias=bias))\n",
    "            self.layers.append(nn.ReLU())\n",
    "        else:\n",
    "            self.layers.append(nn.Dropout(0.2))\n",
    "            self.layers.append(nn.Linear(input_dim, num_classes, bias=bias))\n",
    "            self.layers.append(nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class ImgClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, in_channels, input_dim_fc, out_channels, kernel_sizes, strides=None, dilations=None):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential()\n",
    "        K = len(kernel_sizes)\n",
    "        if not strides:\n",
    "            strides = [1 for _ in range(K)]\n",
    "        if not dilations:\n",
    "            dilations = [1 for _ in range(K)]\n",
    "        assert len(kernel_sizes) == len(strides) == len(dilations), \"lengths of kernel_sizes, strides, dilations should be equal\"\n",
    "        #self.conv1 = nn.Conv2d(in_channels, in_channels, 3)\n",
    "        #self.batch_norm1 = nn.BatchNorm2d(in_channels)\n",
    "        #self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        in_channel_curr = in_channels\n",
    "        for out_channel, kernel, stride, dilation in zip(out_channels, kernel_sizes, strides, dilations):\n",
    "            conv = ConvolutionNet(in_channels=in_channel_curr, out_channels=out_channel,\n",
    "                 kernel_size=kernel, stride=stride, dilation=dilation)\n",
    "            in_channel_curr = out_channel\n",
    "            pool = Pooling(kernel_size=3, stride=2, padding=1)\n",
    "            self.block.append(conv)\n",
    "            self.block.append(pool)\n",
    "        self.fc = FCNet(input_dim=input_dim_fc, num_classes=num_classes, number=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #x = self.conv1(x)\n",
    "        #x = self.batch_norm1(x)\n",
    "        #x = self.maxpool(x)\n",
    "        x = self.block(x)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4591c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR100(\"/archive/turganbay/cifar\", train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR100(\"/archive/turganbay/cifar\", train=False, download=True, transform=transform)\n",
    "N = len(train_dataset)\n",
    "T = int(N*0.9)\n",
    "train_dataset, val_dataset = random_split(train_dataset, [T, N-T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9819f15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "psum = torch.tensor([0.0, 0.0, 0.0])\n",
    "psum_sq = torch.tensor([0.0, 0.0, 0.0])\n",
    "\n",
    "for inputs, lbl in DataLoader(train_dataset, 64):\n",
    "    psum += inputs.sum(axis = [0, 2, 3])\n",
    "    psum_sq += (inputs**2).sum(axis = [0, 2, 3])\n",
    "\n",
    "count = len(train_dataset) * 224 * 224\n",
    "\n",
    "total_mean = psum / count\n",
    "total_var  = (psum_sq / count) - (total_mean ** 2)\n",
    "total_std  = torch.sqrt(total_var)\n",
    "\n",
    "print('Training data stats:')\n",
    "print('- mean: {}'.format(total_mean))\n",
    "print('- std:  {}'.format(total_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b723b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.subset[index]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "\n",
    "    \n",
    "normalize = transforms.Normalize(mean=[0.5073, 0.4868, 0.4410], std=[0.2623, 0.2515, 0.2716])\n",
    "transform_aug = transforms.Compose([\n",
    "    #transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(degrees=5),\n",
    "    #transforms.RandomResizedCrop(size=(224, 224), antialias=True),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = MyDataset(train_dataset, transform=transform_aug)\n",
    "val_dataset = MyDataset(val_dataset, transform=normalize)\n",
    "test_dataset = MyDataset(test_dataset, transform=normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfded4bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/turganbay/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/turganbay/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): VGG(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (9): ReLU(inplace=True)\n",
       "      (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (12): ReLU(inplace=True)\n",
       "      (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (16): ReLU(inplace=True)\n",
       "      (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (19): ReLU(inplace=True)\n",
       "      (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (22): ReLU(inplace=True)\n",
       "      (23): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (24): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (25): ReLU(inplace=True)\n",
       "      (26): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (27): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (29): ReLU(inplace=True)\n",
       "      (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (32): ReLU(inplace=True)\n",
       "      (33): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (34): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (35): ReLU(inplace=True)\n",
       "      (36): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (37): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (38): ReLU(inplace=True)\n",
       "      (39): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (42): ReLU(inplace=True)\n",
       "      (43): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (44): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (45): ReLU(inplace=True)\n",
       "      (46): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (47): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (48): ReLU(inplace=True)\n",
       "      (49): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (50): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (51): ReLU(inplace=True)\n",
       "      (52): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "    (classifier): FCNet(\n",
       "      (layers): Sequential(\n",
       "        (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.2, inplace=False)\n",
       "        (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        (4): ReLU()\n",
       "        (5): Linear(in_features=4096, out_features=100, bias=True)\n",
       "        (6): ReLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 16\n",
    "num_classes = 100\n",
    "epochs = 40\n",
    "\n",
    "#out_channels = [64, 128, 256, 512]\n",
    "#kernel_sizes = [3] * len(out_channels)\n",
    "#model = ImgClassifier(num_classes, 3, 100352, out_channels, kernel_sizes).to(device)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=bs)\n",
    "val_loader = DataLoader(val_dataset, batch_size=bs)\n",
    "test_loader = DataLoader(test_dataset, batch_size=bs)\n",
    "\n",
    "weights = torch.hub.load(\"pytorch/vision\", \"get_model_weights\", name=\"vgg19_bn\")\n",
    "weight_name = [weight for weight in weights][0]\n",
    "weights = get_weight(str(weight_name))\n",
    "model = torch.hub.load(\"pytorch/vision\", \"vgg19_bn\", weights=weight_name)\n",
    "model.classifier = FCNet(input_dim=25088, num_classes=100, number=3)\n",
    "\n",
    "loss_fct = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-02)  \n",
    "\n",
    "grad_accum_steps = 4\n",
    "num_train_epochs = 40\n",
    "total_iters = len(train_loader) // grad_accum_steps * epochs\n",
    "scheduler = transformers.get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=50, num_training_steps=total_iters\n",
    ")\n",
    "\n",
    "# These lines configure model parallelism (use multiple GPUs if available) and specify the target device (e.g., CPU or GPU).\n",
    "model= nn.DataParallel(model)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9a7bfed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139990948"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5735401e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/40:   0%|                                                                                                                                     | 0/2813 [00:00<?, ?batch/s]libibverbs: Warning: couldn't open config directory '/etc/libibverbs.d'.\n",
      "Epoch 0/40:  57%|█████████████████████████████████████████████████████████████████████▌                                                    | 1603/2813 [03:22<02:24,  8.38batch/s]"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i, (images, labels) in tqdm(enumerate(train_loader), desc=f\"Epoch {epoch}/{epochs}\", \n",
    "                            unit=\"batch\", total=len(train_loader)):  \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = loss_fct(outputs, labels) / grad_accum_steps\n",
    "        loss.backward()\n",
    "        if (i + 1) % grad_accum_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "\n",
    "    print(f'Loss: {round(loss.item()*grad_accum_steps, 3)}')\n",
    "            \n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "        print(f'Accuracy {round(correct/total*100, 3)}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb3f0bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_llm",
   "language": "python",
   "name": "env_llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
